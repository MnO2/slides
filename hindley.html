<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }

      .red    { color: #FF4943; }
      .gray   { color: #787878; }
      .green  { color: #87A558; }
      .blue   { color: #41C8F0; }
      .yellow { color: #DBEC62; }
      .large p { font-size: 4em; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Hindley Milner Type System

![h-and-m](image/HM.png)

### Paul Meng

---

## What part of types don't you understand

![what-part](image/what_part_of_types_dont_you_understand.jpg)
--

![wtf](image/jackie-chan-wtf.png)


---


## Hindley Milner Type Rules

![syntax-directed-hm](image/syntax_directed_hm.jpg)

* It's quite daunting at first glance, and you just can't understand any single bit of it if you are not from a similar background.

---

## Why is it so hard ?

* The language (predicate logic) used to describe it is the language used in academy. The purpose is to be exact but not for comprehension.
--

* Academic people just use lots of jargon in communication for brevity. As a newcomer you have to pickup the intuition and jargon.
--

* Sometimes the form you see is not the form where you could read out an algorithm. (Not syntax-directed form)
--

* You don't understand why it has to be in a simplified formal language when it comes to a tutorial. What it has to do with Haskell? Can I just learn the model for the real world Haskell?
  - Haskell's type system is much more complicated. But as a ML family member, it is quite similar.
--

* It's unclear why do we need it? What's the motivation for the researcher developed such kind of system. We just accepted it as a part of the programming language?

---

## Using predicate logic as lightly as possible

* This talk tries to derive it from intuition and examples.
* Trade off exactnesss for easier comprehension.
* We would define the simplified one and gradually move to the harder ones.


---

## Starting off with an observation

* In McDonald, a simple burger is consist of breads and beef.
* Let's "formulate" this observation with a simple language

``` haskell
data Term = Var Name              -- x
          | Lit Int               -- 3
          | App Term Term         -- f x
          | Lam Name Term         -- \x -> x
          | Ann Term Rho          -- (f x) :: Int
```

* I would really love to call this language as "System Mc", but that is actually a misnomer. It's called lambda calculus.
* Of course anyone could see this it is unsafe. You could put anything into the x, y and makes it a burger. (Cockroach?)

``` haskell
(\x -> \y -> burger) bread beef
```

---

## Simply typed lambda calculus

* Therefore we extend this language, to add types.

* Given a built-in rule (We, the programmer as the oracle, declare this as the Hamburger theorem.)

$$
Bread \rightarrow Beef \rightarrow Burger
$$

* Then we could check if this program follows the rule of the theorem.

``` haskell
(\x: Bread -> \y: Beef -> burger) bread beef
```


---

## It's not only for type checking.

* Because the rule is not ambiguous, we could run the rule checking "backward" to learn the type of a term, with all the constant's type known.
* With human brains, we could see the answer by the rules in a glance, but how to derive an algorithm?

* Rule:
$$
Bread \rightarrow Beef \rightarrow Burger
$$

* Missing y type

``` haskell
((\x: Bread -> \y: ??? -> burger) :: Bread -> Beef -> Burger) bread) beef
```

---

## Let's start from something vague

* We would like to have a `typecheck` function, when given a value level `Term` expression, return a inferred `Rho` type. And since the inference involves some states, we have to put it in a Monad `Tc`.
* `typecheck` calls `inferRho` to states its implementation.

``` haskell
typecheck :: Term -> Tc Rho
typecheck e = inferRho e
```

* We allocate a type variable, put it into another box stating that we are in the inference phase.
* After the `tcRho` finished, we read its inferred type stored in `ref`

``` haskell
inferRho :: Term -> Tc Rho
inferRho expr
  = do { ref <- newTcRef (error "inferRho: empty result")
       ; tcRho expr (Infer ref)
       ; readTcRef ref }
```
* Then what's our target?

---

## Target: Reformulate our problem

* The essense of type inference is walking the abstract syntax tree and collect constraints we must satisfy. Therefore we would reformulate the problem to so-called "unification problem".

--

* Unification problem example

$$
T_1 \rightarrow T_1 * bool = (T_3 + int) \rightarrow T_2
$$

--

* Then we have a solution 

$$ \\{ T_1 \Rightarrow T_3 + int, T_2 \Rightarrow (T_3 + int) * bool \\} $$

--

* Our target is to collect Constraints

``` haskell
a -> b -> c = Bread -> Beef -> Burger
a = Bread
c = Burger
```

--
* Then solving this contraint giving us `b = Beef`

---

## Unification problem

* In academic language, we could describe Unification problem like following.

--

* A set of equations, with a definition of what is equal (equivalence relation in mathematical terminology)

$$ \\{ a = x \\} $$


* A substitution 

$$ \\{ x \rightarrow a \\} $$

* And after applying the substitution, both sides of the equation are equal

$$ a \\{ x \rightarrow a \\}  =  x \\{ x \rightarrow a \\} $$

* The substitution is called unifier


---

## Formulation: Type variable expansion

* But notice that we don't have "type variable" either in the syntax or parsed abstract syntax tree.

``` haskell
((\x: Bread -> \y: ??? -> burger) :: Bread -> Beef -> Burger) bread) beef
```

--

* We have to figure out a way to generate the "type variables". Or in more complicated form with arrow (->), to make it a type expression.

--

* Grammar for generating type variable expansion in type inference. With this grammar, we have the "meta type variable" for generating type expressions. 

$$ 
\rho ::= \tau \\\
\tau ::= \tau \rightarrow \tau \vert T \\\
$$

* That is the `Rho` we mentioned.

``` haskell
inferRho :: Term -> Tc Rho
```

---
## Formulation: Type variable expansion

$$ 
\rho ::= \tau \\\
\tau ::= \tau \rightarrow \tau \vert T \\\
$$

* And now we have the "type expression" defined.

``` haskell
type Rho   = Type
type Tau   = Type

data Type = Fun    Type Type ›    -- Function type
    ›     | TyCon  TyCon      ›   -- Type constants
    ›     | TyVar  TyVar      ›   -- Always bound by a ForAll

data TyVar
  = SkolemTv String Uniq›   -- A skolem constant; the String is•
```

---

## Tree Walking

``` haskell
inferRho expr
  = do { ref <- newTcRef (error "inferRho: empty result")
       ; tcRho expr (Infer ref)
       ; readTcRef ref }
```

![slc-ex1](image/slc_ex1.svg)

---

## Tree Walking

``` haskell
tcRho (App fun arg) exp_ty
  = do { fun_ty <- inferRho fun
       ; (arg_ty, res_ty) <- unifyFun fun_ty
       ; ....
       }
```

* Wait.. What is `unifyFun` ?

![slc-ex1](image/slc_ex1.svg)

``` haskell
unifyFun :: Rho -> Tc (Sigma, Rho)
--      (arg,res) <- unifyFunTy fun
-- unifies 'fun' with '(arg -> res)'
unifyFun (Fun arg res) = return (arg,res)
unifyFun tau           = do { arg_ty <- newTyVarTy
                            ; res_ty <- newTyVarTy
                            ; unify tau (arg_ty --> res_ty)
                            ; return (arg_ty, res_ty) }
```

---

## Robinson's Unification Algorithm in Action

* `unify` the symbol `tau` with the formula of `a1 -> a2`
* That is, Contraint $\tau = a1 \rightarrow a2$
* We allocate type variables with `newTyVarTy`

``` haskell
unifyFun :: Rho -> Tc (Rho, Rho)
unifyFun (Fun arg res) = return (arg,res)
unifyFun tau           = do { arg_ty <- newTyVarTy
                            ; res_ty <- newTyVarTy
                            ; unify tau (arg_ty --> res_ty)
                            ; return (arg_ty, res_ty) }
```

---

## Robinson's Unification Algorithm in Action

* If the argument is `Fun`, then first `unify` the argument, then `unify` its body.

``` haskell
unify :: Tau -> Tau -> Tc ()
unify (Fun arg1 res1)
      (Fun arg2 res2)
  = do { unify arg1 arg2; unify res1 res2 }
```

--

* If the type is `badType`, then indicate it is a compilation error.

``` haskell
unify ty1 ty2
  | badType ty1 || badType ty2  -- Compiler error
  = failTc (text "Panic! Unexpected types in unification:" <+>
            vcat [ppr ty1, ppr ty2])
```

--

* If it is a constant type, then check if they are matched.

``` haskell
unify (TyCon tc1) (TyCon tc2)•
  | tc1 == tc2•
  = return ()
```

---

## Robinson's Unification Algorithm in Action

``` haskell
unify (TyVar tv1)  (TyVar tv2)  | tv1 == tv2 = return ()
```
--

* Otherwise, failed to check the program.

``` haskell
unify ty1 ty2 = failTc (text "Cannot unify types:" <+> vcat [ppr ty1, ppr ty2])
```

--

* With unification algorithm, we could get back to the tree walking.

---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (App fun arg) exp_ty
  = do { fun_ty <- inferRho fun
       ; (arg_ty, res_ty) <- unifyFun fun_ty
       ; checkRho arg arg_ty
       ; instRho res_ty exp_ty }

checkRho :: Term -> Rho -> Tc ()
checkRho expr ty = tcRho expr (Check ty)

instRho :: Rho -> Expected Rho -> Tc ()
instRho t1 (Check t2) = unify t1 t1
instRho t1 (Infer r) = writeTcRef r t1
```

---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (Lam var body) (Infer ref)
  = do { var_ty  <- newTyVarTy
       ; body_ty <- extendVarEnv var var_ty (inferRho body)
       ; writeTcRef ref (var_ty --> body_ty) }
tcRho (Lam var body) (Check exp_ty)
  = do { (var_ty, body_ty) <- unifyFun exp_ty•
       ; extendVarEnv var var_ty (checkRho body body_ty) }
```
*  `extendVarEnv` put a record marking a term's type as Rho, for the lookup afterward.

---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (Var v) exp_ty•
  = do { v_rho <- lookupVar v•
       ; instRho v_rho exp_ty }
```

* `lookupVar` lookup a Term's type by checking if the record exists.

---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (Lit _) exp_ty
  = instRho BurgerType exp_ty
```

* Constant type is trivial.

---

## Tree Walking

* After tree walking, we should get the type of the the whole expression

``` haskell
inferRho expr
  = do { ref <- newTcRef (error "inferRho: empty result")
       ; tcRho expr (Infer ref)
       ; readTcRef ref }
```

--
* However, this type system is too simple. No generalization.

---

## The defect of simply typed lambda calculus

* Suppose that we would like to have a generalized version of hamburger making.
* Hamburger is not only for meat, but could be fish. And we could use Rice for Bread

``` haskell
(\f -> ((f bread meat), (f bread fish), (f rice meat)) ) (make)
```

* The trick discovered by Hindley and Milner independently is to add a "let" into grammar
* It's called Let polymorphism.

``` haskell
let f = make in ((f bread meat), (f bread fish), (f rice meat))
```

* The type system is also generalized. For the language to describe "type" (type expression) is added with polytype, that is `forall a. a`

``` haskell
forall a b. a -> b -> Burger
```


---

## Let Polymorphism

* First of all, we modified the grammar for value level expression.

``` haskell
data Term = Var Name              -- x
          | Lit Int               -- 3
          | App Term Term         -- f x
          | Lam Name Term         -- \ x -> x
          | Let Name Term Term    -- let x = f y in x+1
          | Ann Term Sigma        -- (f x) :: Int
```

* We add a "Let Name Term Term" here
* Polytypes (We use Sigma here) are types containing variables bound by one or more for-all quantifiers
* Then we extend the structure we previously defined one by one.

---

## Let Polymorphism

* Then we extends the type expression.

$$ 
\sigma := \forall a. \rho \\\
\rho ::= \tau \\\
\tau::= \tau \rightarrow \tau| T \\\
$$

* We add another meta variable "Sigma" for type level expansion.
* And "BoundTv" to capture the "forall" polytype.

``` haskell
type Sigma = Type
type Rho   = Type›  -- No top-level ForAll
type Tau   = Type›  -- No ForAlls anywhere

data Type = ForAll [TyVar] Rho›   -- Forall type
    ›     | Fun    Type Type ›    -- Function type
    ›     | TyCon  TyCon      ›   -- Type constants
    ›     | TyVar  TyVar      ›   -- Always bound by a ForAll
    ›     | MetaTv MetaTv     ›   -- A meta type variable

data TyVar
  = BoundTv String› ›   -- A type variable bound by a ForAll
  | SkolemTv String Uniq›   -- A skolem constant; the String is•
```


---

## Instantiation

* With the polytype introduced, an acompanying concept is introduced, it's called instantiation.

--

* It's replacing "type variable" identified by "schema" with fresh "type variable"

--

* The left hand side could instantiate the right hand side.

$$
a \leq Int \\\
a \rightarrow a \leq Int \rightarrow Int \\\
a \rightarrow a \leq b \rightarrow b \\\
\forall a. a \rightarrow a \leq c \rightarrow c \\\
$$

--

* How do we tell if one of the polytype could subsume another?

--

* Skolemise one of the polytype, and see if the other polytype could instantiate it after skolemization.


---

## Instantiation

* Substitute the type variable appears in `forall`, with newly instantiated type variables, they are not the same from any other type variables.

``` haskell
instSigma :: Sigma -> Expected Rho -> Tc ()
-- Invariant: if the second argument is (Check rho),
-- ›      then rho is in weak-prenex form
instSigma t1 (Check t2) = unify t1 t2
instSigma t1 (Infer r)  = do { t1' <- instantiate t1
                             ; writeTcRef r t1' }

instantiate :: Sigma -> Tc Rho
-- Instantiate the topmost for-alls of the argument type
-- with flexible type variables
instantiate (ForAll tvs ty)•
  = do { tvs' <- mapM (\_ -> newMetaTyVar) tvs
       ; return (substTy tvs (map MetaTv tvs') ty) }
instantiate ty
```

---

## Generalization

* Another concept introduced is called "generalization.
* It's replacing a "type variable" into a "polytype". Think it like a C++ template that could generate function specialization.


``` haskell
inferSigma :: Term -> Tc Sigma
inferSigma e
   = do { exp_ty <- inferRho e
        ; env_tys <- getEnvTypes
        ; env_tvs <- getMetaTyVars env_tys
        ; res_tvs <- getMetaTyVars [exp_ty]
        ; let forall_tvs = res_tvs \\ env_tvs
        ; quantify forall_tvs exp_ty }


checkSigma :: Term -> Sigma -> Tc ()
checkSigma expr sigma
= do { (skol_tvs, rho) <- skolemise sigma
     ; checkRho expr rho
     ; env_tys <- getEnvTypes
     ; esc_tvs <- getFreeTyVars (sigma : env_tys)
     ; let bad_tvs = filter (`elem` esc_tvs) skol_tvs
     ; check (null bad_tvs)
             (text "Type not polymorphic enough") }
```


---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (App fun arg) exp_ty
  = do { fun_ty <- inferRho fun
       ; (arg_ty, res_ty) <- unifyFun fun_ty
       ; checkSigma arg arg_ty
       ; instSigma res_ty exp_ty }
```

---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (Let var rhs body) exp_ty
  = do { var_ty <- inferSigma rhs
       ; extendVarEnv var var_ty (tcRho body exp_ty) }
```

---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (Lit _) exp_ty
  = instSigma intType exp_ty
```

---

![slc-ex1](image/slc_ex1.svg)

``` haskell
tcRho (Var v) exp_ty•
  = do { v_sigma <- lookupVar v•
       ; instSigma v_sigma exp_ty }

tcRho (Ann body ann_ty) exp_ty
   = do { checkSigma body ann_ty
        ; instSigma ann_ty exp_ty }
```


---


## Change in Unification

``` haskell
unify (MetaTv tv1) (MetaTv tv2) | tv1 == tv2 = return ()
unify (MetaTv tv) ty = unifyVar tv ty
unify ty (MetaTv tv) = unifyVar tv ty
```

--

``` haskell
unifyVar :: MetaTv -> Tau -> Tc ()
-- Invariant: tv1 is a flexible type variable
unifyVar tv1 ty2        -- Check whether tv1 is bound
  = do { mb_ty1 <- readTv tv1•••
       ; case mb_ty1 of
           Just ty1 -> unify ty1 ty2
           Nothing  -> unifyUnboundVar tv1 ty2 }
```

---

## Change in Unification

``` haskell
unifyUnboundVar :: MetaTv -> Tau -> Tc ()
-- Invariant: the flexible type variable tv1 is not bound
unifyUnboundVar tv1 ty2@(MetaTv tv2)
  = do { -- We know that tv1 /= tv2 (else the•
         -- top case in unify would catch it)
         mb_ty2 <- readTv tv2
       ; case mb_ty2 of
           Just ty2' -> unify (MetaTv tv1) ty2'
           Nothing  -> writeTv tv1 ty2 }•

unifyUnboundVar tv1 ty2
  = do { tvs2 <- getMetaTyVars [ty2]
       ; if tv1 `elem` tvs2 then
            occursCheckErr tv1 ty2
         else
            writeTv tv1 ty2 }
```


---

## What's the defect of hindley-milner type system?

* The worst case of the time complexity is exponential.

``` haskell
let b = true in
let f0 = λx. x+1 in
let f1 = λx. if b then f0 else λy.x y in
let f2 = λx. if b then f1 else λy.x y in
.
.
let fn = λx. if b then fn−1 else λy.x y in
```

* Reference type would break the type system's soundness

--
* But we are going to stres on the last one, which is solvable by higher-rank type system
* Even with the addition of "let", it only tells the system to "generalize" at the syntax where "let" happened.

```haskell
(\f -> (f "foo", f 13)) (\x -> x)
```


---

## Higher-rank type system 

* To solve the problem of let polymorphism where it is not generalized at the "argument"
* Consider the following use case.

```haskell
make :: (forall a. a -> a) -> a -> b -> Burger

makeBurgers :: (forall a. a -> a) -> (Burger, Burger)
makeBburgers g = let f x = (x bread beef, x bread fish)
                 in f g

makeBurgers make
```

* We add a type annotation to help the type system to generalize at the argument, without a big change in hindley-milner type system.


---

## Change in Value and Type Expression

* Add type annotation in the abstraction rule, i.e. ALam

``` haskell
data Term = Var Name              -- x
          | Lit Int               -- 3
          | App Term Term         -- f x
          | Lam Name Term         -- \ x -> x
          | ALam Name Sigma Term  -- \ x -> x
          | Let Name Term Term    -- let x = f y in x+1
          | Ann Term Sigma        -- (f x) :: Int
```

--

* Make $\rho$ could derive $\sigma \rightarrow \sigma$
$$ 
\rho ::= \tau | \sigma \rightarrow \sigma^{\prime} \\\
\tau::= \tau \rightarrow \tau| T \\\
$$


---

## Change in Inference

``` haskell
tcRho (ALam var var_ty body) (Check exp_ty)
  = do { (arg_ty, body_ty) <- unifyFun exp_ty•
       ; subsCheck arg_ty var_ty
       ; extendVarEnv var var_ty (checkRho body body_ty) }

tcRho (ALam var var_ty body) (Infer ref)
  = do { body_ty <- extendVarEnv var var_ty (inferRho body)
       ; writeTcRef ref (var_ty --> body_ty) }
```

--
* What is `subsCheck` ?


---

## Subsumption

* A trick to make different poly type expression be equivalent.

--

* With the type annotation added, the type derived from hindley milner.
$$
\forall a. a \rightarrow (\forall b.b \rightarrow b)
$$

* And it is actually equivalent to, but the type checker doesn't aware of that by the previous rules
$$
\forall ab. a \rightarrow b \rightarrow b
$$

* We hope that the following to hold. That is, get some tricks to make the generalized types being "subsumed" to another generalized type.
$$
\forall a. a \rightarrow (\forall b.b \rightarrow b) \leq \forall ab. a \rightarrow b \rightarrow b \\\
\forall a. a \rightarrow (\forall b.b \rightarrow b) \geq \forall ab. a \rightarrow b \rightarrow b \\\
$$


---

## Subsumption

* We define the prenex form: With all of the "forall" at the "head" of the type annotation.

--

* And the prenex conversion is like the following

$$
\forall a. a \rightarrow (\forall b.b \rightarrow b)
$$

to 

$$
\forall ab. a \rightarrow b \rightarrow b
$$

--
* It's sometimes called Skolemization.

--
* The technique is to convert the annotated type expression into prenex form and see if one is more generalized than the other.

---

## Skolemization

* Recursivelly floating out the `Forall`

``` haskell
skolemise :: Sigma -> Tc ([TyVar], Rho)
-- Performs deep skolemisation, retuning the•
-- skolem constants and the skolemised type
skolemise (ForAll tvs ty)›  -- Rule PRPOLY
  = do { sks1 <- mapM newSkolemTyVar tvs
       ; (sks2, ty') <- skolemise (substTy tvs (map TyVar sks1) ty)
       ; return (sks1 ++ sks2, ty') }
skolemise (Fun arg_ty res_ty)›  -- Rule PRFUN
  = do { (sks, res_ty') <- skolemise res_ty
       ; return (sks, Fun arg_ty res_ty') }
skolemise ty ›  ›   ›   -- Rule PRMONO
  = return ([], ty)
```

---

## Subsumption

* It check if `sigma2` is at least as polymorphic as `sigma1 -> exp`

``` haskell
subsCheck :: Sigma -> Sigma -> Tc ()
subsCheck sigma1 sigma2        -- Rule DEEP-SKOL
  = do { (skol_tvs, rho2) <- skolemise sigma2
       ; subsCheckRho sigma1 rho2
       ; esc_tvs <- getFreeTyVars [sigma1,sigma2]
       ; let bad_tvs = filter (`elem` esc_tvs) skol_tvs
       ; check (null bad_tvs)
               (vcat [text "Subsumption check failed:",
                      nest 2 (ppr sigma1),
                      text "is not as polymorphic as",
                      nest 2 (ppr sigma2)])
```

---

## Subsumption

* `rho2` is in prenex form

``` haskell
subsCheckRho :: Sigma -> Rho -> Tc ()
subsCheckRho sigma1@(ForAll _ _) rho2›   -- Rule SPEC
  = do { rho1 <- instantiate sigma1
       ; subsCheckRho rho1 rho2 }
subsCheckRho rho1 (Fun a2 r2)            -- Rule FUN
  = do { (a1,r1) <- unifyFun rho1; subsCheckFun a1 r1 a2 r2 }
subsCheckRho (Fun a1 r1) rho2            -- Rule FUN
  = do { (a2,r2) <- unifyFun rho2; subsCheckFun a1 r1 a2 r2 }
subsCheckRho tau1 tau2                   -- Rule MONO
  = unify tau1 tau2    -- Revert to ordinary unification

subsCheckFun :: Sigma -> Rho -> Sigma -> Rho -> Tc ()
```

---

## Wrapping up

* The hindley milner system is the foundation of Haskell98 standrad
* Higher rank corresponds to GHC's extension -XHigher-Rank

---

class: center, middle, large

Thank you


    </textarea>
    <script>
        var is_local = (window.location.hostname == "localhost");

        function add_script(src, callback) {
            var s = document.createElement("script");
            s.setAttribute("src", src);
            s.onload = callback;
            document.body.appendChild(s);
        }

        function load_slides() {
            var query = window.location.search.substring(1);
            var slideshow = remark.create();

            // Setup MathJax
            MathJax.Hub.Config({
                tex2jax: {
                    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processescapes: true,
                },
                jax: ["input/TeX","output/HTML-CSS"],
                displayAlign: "left",
                displayIndent: "2em"
            });


            MathJax.Hub.Configured();
        }

        if (is_local) {
            add_script("javascript/remark-latest.min.js", function() {
                add_script("javascript/MathJax.js", load_slides);
            });
        } else {
            add_script("https://gnab.github.io/remark/downloads/remark-latest.min.js", function() {
                add_script("http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured", load_slides);
            });
        }
    </script>
  </body>
</html>
